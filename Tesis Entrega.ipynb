{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas importantes\n",
    "\n",
    "* Métricas a considerar para la evaluación de desempeño:\n",
    "\n",
    "    * Reward: ingresos - penalizaciones (es la métrica principal).\n",
    "\n",
    "    * Penalty.\n",
    "\n",
    "    * Cantidad de pedidos con atraso.\n",
    "\n",
    "    * Cantidad de pedidos rechazados.\n",
    "\n",
    "\n",
    "* Parámetros de instance:\n",
    "\n",
    "    * Tamaño del área de servicio: de 5 a 8 km de arista, aproximadamente el diametro de la comuna de Santiago Centro.\n",
    "    \n",
    "    * N° vehículos y de clientes: aproximadamente 25 clientes por vehículo es lo que considera una instancia real del problema de Gasco.\n",
    "\n",
    "    * dod: mayor a 0.8-0.9.\n",
    "\n",
    "    * t_max: 7 horas.\n",
    "\n",
    "    * t_service: 7 a 10 min.\n",
    "\n",
    "    * t_window: 30 min.\n",
    "\n",
    "    * categorías de clientes: en este caso se considera una categoría de cliente.\n",
    "\n",
    "    * penalty factor: entre 10 y 15 parece adecuado. 15 es un penalty donde al atrasarse 15 min ya se pierde por penalización lo equivalente a un cliente.\n",
    "\n",
    "    * t_delta: entre 5 y 10 min parece adecuado. Entre mayor es el t_delta, menos decisiones se toman por lo que el algoritmo se ejecuta más rápido.\n",
    "\n",
    "    * velocidad: 30 km/h +- 5 km/h es adecuado en el contexto urbano.\n",
    "\n",
    "    * puntos de espera: 3 puntos de espera es razonable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del modelo\n",
    "\n",
    "**Instancia:**\n",
    "\n",
    "* Se tiene un área de servicio donde un set de varios vehículos parten y terminan en un depósito. Los clientes realizan pedidos hasta un t_max.\n",
    "\n",
    "* Los vehículos (con capacidad y autonomía ilimitada) tienen una velocidad determinística o estocástica durante cada tramo que recorren. Cada vez que recorre un tramo entre dos localizaciones se samplea una velocidad (con una distribución de probabilidad como lognormal si se está trabajando con el caso estocástico). Esto implica que la duración en los tiempos de viaje puede ser estocástica.\n",
    "\n",
    "* Existen clientes tempranos (t_arrival = 0) que son conocidos al inicio del problema y clientes tardíos (t_arrival > 0) que van apareciendo dinámicamente durante el día, es decir, en instantes y ubicaciones aleatorias. Todos los clientes tienen un tiempo de servicio t_service conocido, fijo e igual para cada uno. Además, cada cliente tiene una ventana de tiempo de 30 min desde que se confirmó su pedido para ser atendido sin penalización. Por último, cada cliente pertenece a una categoría que indica el nivel de reward que entrega al ser atendido.\n",
    "\n",
    "* Nota: todos los clientes tienen un id único para cada realización.\n",
    "\n",
    "* Nota: en un contexto real deberían haber entre 3 y 4 camiones en un área equivalente a Santiago Centro y deberían haber entre 145 y 255 clientes. Además la velocidad del vehículo debería rondar los 30 o 35 km/h con una desviación estándar de 5 km/h.\n",
    "\n",
    "**Estados:**\n",
    "\n",
    "* Un estado tiene los siguientes elementos:\n",
    "\n",
    "    * Plan de ruta de cada vehículo (que incluye la posición actual del vehículo en la posición 0 de la lista, y el depot en la posición final de la lista).\n",
    "\n",
    "    * Un (posible) cliente aleatorio.\n",
    "    \n",
    "    * Tiempo t.\n",
    "\n",
    "* En el estado inicial (t = 0) los vehículos se encuentran en el depot y el cliente aleatorio es uno de los clientes tempranos (t_arrival = 0).\n",
    "\n",
    "* En el estado terminal los vehículos se encuentran en el depot, no quedan clientes por atender, y todos los clientes del día fueron vistos (confirmados o rechazados).\n",
    "    \n",
    "* Los puntos de decisión se dan en los momentos en que llega un nuevo cliente o bien luego de un intervalo de tiempo t_delta en el que no ha llegado ningún cliente.\n",
    "\n",
    "**Acciones**\n",
    "\n",
    "* Una acción en un punto de decisión incluye la confirmación o rechazo del cliente aleatorio y una decisión de movimiento; actualización del plan de ruta a cada vehículo.\n",
    "\n",
    "* Las acciones posibles para cada vehículo se determinan considerando las siguientes condiciones:\n",
    "\n",
    "    * Si se superó el t_max del problema o bien quedan clientes por atender en la ruta, se puede continuar con el mismo plan de ruta.\n",
    "\n",
    "    * Si no quedan clientes por atender en el plan de ruta y no se ha alcanzado t_max se puede enviar el vehículo a cualquier idlepoint sólo mientras éste se encuentre suficientemente lejos del vehículo (a más de 1/3 de la diagonal del área de servicio). Lo anterior se justifica en que si un vehículo se encuentra en un cliente muy cercano a un idlepoint, no se espera que mover el vehículo tan poco tenga un aporte significativo al reward. Además, esta medida permite disminuir el espacio de acciones.\n",
    "\n",
    "    * Si el vehículo se encuentra en el depot, en un idlepoint o en un cliente, no tiene clientes por atender en su plan de ruta y no se ha superado t_max, se puede esperar en la posición hasta el siguiente punto de decisión.\n",
    "\n",
    "    * Si en el punto de decisión existe un cliente aleatorio, se puede considerar su inserción en alguna de las 5 posiciones menos costosas (en términos de distancia) en el plan de ruta de alguno de los 3 vehículos más cercanos. Lo anterior se justifica en la necesidad de eliminar las acciones evidentemente malas para disminuir el espacio de acciones. Si la siguiente posición en la ruta es un idlepoint, éste se puede sacar de la ruta e insertar al cliente en su lugar.\n",
    "\n",
    "    * Otras restricciones importantes: un cliente aleatorio sólo puede ser insertado en un vehículo. Todos los clientes tempranos (t_arrival = 0) deben ser atendidos, es decir, no existe la opción de rechazar a dichos clientes.\n",
    "\n",
    "    * Nota: eventualmente si el espacio de acciones sigue siendo demasiado grande se podrían quitar los idlepoints.\n",
    "\n",
    "**Estado-Accion:**\n",
    "\n",
    "* Un objeto State-Action tiene los siguientes elementos (dado que pertenece a una clase hija de State): \n",
    "\n",
    "    * Plan de ruta de cada vehículo (que incluye la posición actual del vehículo en la posición 0 de la lista, y el depot en la posición final de la lista).\n",
    "\n",
    "    * Un (posible) cliente aleatorio.\n",
    "    \n",
    "    * Tiempo t.\n",
    "\n",
    "* Este objeto permite calcular los valores asociados a tomar una acción en cierto estado. Para esto se extrae de este elemento el vector de features.\n",
    "\n",
    "* Nota: Si la diferencia de la escala entre los features demasiado alta se deberían estandarizar los features para que no afecte la regresión.\n",
    "\n",
    "**Transición y Rewards:**\n",
    "\n",
    "* La transición entre dos estados está dada por la llegada de un cliente aleatorio o bien por el paso de t_delta minutos sin la llegada de un cliente. En este intervalo de tiempo se ejecuta el plan de ruta de cada vehículo, que considera para cada tramo entre localizaciones el sampleo de una velocidad que es aleatoria.\n",
    "\n",
    "* En la transición a otro estado se percibe un reward que está compuesto por la ganancia asociada a aceptar al cliente menos la penalización por los clientes que fueron o están siendo atendidos fuera de su ventana de tiempo.\n",
    "\n",
    "* La penalización por minuto de atraso está dada por una función monótona no decreciente (ej. logaritmo).\n",
    "\n",
    "**Algoritmos para la solución del MDP:**\n",
    "\n",
    "1. Política miope 1 (cheapest insertion algorithm):\n",
    "\n",
    "    * Para este algoritmo el take action considera lo siguiente: siempre aceptar al cliente aleatorio e insertarlo en alguna posición dentro de cualquiera de los planes de ruta de forma que el aumento en distancia total por recorrer sea el mínimo.\n",
    "\n",
    "    * Nota con respecto al take action anterior: se observa que se tiende a asignar a todos los clientes al vehículo que ya tiene clientes en la ruta (habitualmente al primer vehículo). Esto se genera porque en la mayoría de los casos es menos costoso asignar el cliente al vehículo que ya tiene una ruta con clientes a mover un vehículo del depot para atenderlo. Esto afecta claramente el reward obtenido por el algoritmo, dado que el vehículo que concentra todos los pedidos tiende a llegar tarde.\n",
    "\n",
    "    * Para solucionar lo anterior se consideró un take action alternativo similar: siempre aceptar al cliente aleatorio e insertarlo en alguna posición dentro del plan de ruta del vehículo más cercano de forma que el aumento en distancia total por recorrer sea el mínimo. Esto permite que la asignación de clientes sea más balanceada entre los vehículos, lo que mejora el reward del algoritmo.\n",
    "\n",
    "    * Nota importante con respecto a los take action: se observa que con cualquiera de los take action anteriores los clientes tempranos (t_arrival = 0) tienden a asignarse a sólo un vehículo. Esto se produce porque el algoritmo los asigna siempre a un mismo vehículo. Cuando las instancias son muy grandes y la cantidad de clientes tempranos es alta esto es un problema debido a que se colapsa uno de los vehículos, mientras los demás se encuentran relativamente inutilizados. Eventualmente se debería considerar alguna medida que balancee la asignación de estos clientes; por ejemplo, si al insertar en cierto vehículo se está generando un atraso, se intenta insertar en el siguiente vehículo.\n",
    "\n",
    "    * Nota extra: queda por verificar con Gasco cuál es la política que aplican.\n",
    "\n",
    "2. Política inteligente (AVI algorithm):\n",
    "\n",
    "    * Este es un algoritmo de AVI que utiliza una aproximación lineal de la value function. Las decisiones se toman con una política inicialmente arbitraria que se va actualizando a medida que el algoritmo aprende y actualiza la value function. Para hacer el aprendizaje más eficaz durante el entrenamiento se utiliza una estrategia e-greedy para la toma de decisiones para promover la exploración de acciones buenas. La estrategia e-greedy considera la elección aleatoria entre la 2da, 3ra, 4ta y 5ta mejor opción en cierto punto de decisión.\n",
    "    \n",
    "    * El take action del algoritmo considera lo siguiente: para cierto par state-action se aplica la política de decisión actual, que selecciona la acción que maximiza el value-to-go calculado con la aproximación lineal de la value function. Esto permite iterativamente actualizar los parámetros de la value function e ir mejorando la toma de decisiones hasta obtener una política de decisión que converge a la óptima.\n",
    "\n",
    "    * Value Function Approximation: la actualización de los parámetros se realiza en base a RLS propuesto por powell 2011 (p. 350). Eventualmente se podría considerar la opción de utilizar la versión de RLS para datos no estacionarios. Esto se explica porque la política se actualiza iterativamente, por lo que los values no son estacionarios (el promedio converge a cierto valor según la política).\n",
    "\n",
    "    * Nota: El algoritmo del código está basado en el algoritmo que plantea Sutton en la sección 5.4, p. 104 (On policy first visit MC control for e-soft policies), utilizando RLS según los planteado por Powell en la sección 9.3.1 p. 350.\n",
    "\n",
    "3. Política miope 2 (cluster algorithm):\n",
    "\n",
    "    * Este algoritmo considera la clusterización del área de servicio en zonas fijas donde existe un grupo de vehículos asignado exclusivamente a cada zona.\n",
    "\n",
    "    * Para este algoritmo el take action considera los siguiente: siempre aceptar al cliente aleatorio y, dependiendo de su zona, insertarlo con cheapest insertion (en términos de distancia recorrida) en alguna posición dentro de cualquiera de los planes de ruta de alguno de los vehículos asignados a su zona."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías generales\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "from math import log, factorial, ceil\n",
    "import heapq\n",
    "\n",
    "# librerías para graficar y animar\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# librería para entrenar una regresión lineal\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# librería para guardar los resultados\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(loc_list):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Función auxiliar para medir la distancia euclideana de una secuencia de localizaciones del área de servicio. Por ejemplo, para una secuencia [o1, o2, o3], entrega \n",
    "        la distancia euclideana entre o1 y o2 + la distancia euclideana entre o2 y o3. Si se entregan dos localizaciones, se mide la distancia sólo entre los dos objetos.\n",
    "\n",
    "    Parámetros:\n",
    "\n",
    "        * loc_list: secuencia de localizaciones (objetos con atributo posición) \n",
    "\n",
    "    Return:\n",
    "\n",
    "        * distance: distancia euclideana entre las localizaciones entregadas.\n",
    "    '''\n",
    "\n",
    "    # crear un array de coordenadas\n",
    "    coord = np.array([loc.pos for loc in loc_list])\n",
    "\n",
    "    # calcular la distancia total a lo largo de la secuencia de objetos\n",
    "    delta = coord[1:] - coord[:-1]\n",
    "    distances = np.hypot(delta[:, 0], delta[:, 1])\n",
    "    total_distance = np.sum(distances)\n",
    "\n",
    "    return total_distance\n",
    "\n",
    "def sampleVel(instance):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Función auxiliar para samplear la velocidad de un vehículo en un tramo (según el tipo de instancia puede ser determinista o estocástica).\n",
    "\n",
    "    Parámetros:\n",
    "\n",
    "        * instance: objeto de la clase Instance.\n",
    "\n",
    "    Return:\n",
    "\n",
    "        * vel: velocidad generada para el vehículo en el tramo.\n",
    "    '''\n",
    "\n",
    "    if instance.stoc_travel_time:\n",
    "        mean = log(instance.vel_mean ** 2 / (instance.vel_mean ** 2 + instance.vel_std ** 2) ** (1/2))\n",
    "        std = (log(1 + (instance.vel_std ** 2 / instance.vel_mean ** 2))) ** (1/2)\n",
    "        vel = round(random.lognormvariate(mean, std), 2)\n",
    "    else:\n",
    "        vel = instance.vel_mean\n",
    "\n",
    "    return vel\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(values_observed, values_predicted):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Función auxiliar para calcular el error absoluto porcentual medio.\n",
    "\n",
    "    Parámetros:\n",
    "\n",
    "        * values_observed: secuencia con los valores observados (True).\n",
    "        * values_predicted: secuencia con los valores predichos.\n",
    "\n",
    "    Return:\n",
    "\n",
    "        * mape: mean absolute percentage error.\n",
    "    '''\n",
    "\n",
    "    ape = [abs(value_observed - value_predicted) / max(1, abs(value_observed)) for value_observed, value_predicted in zip(values_observed, values_predicted)]\n",
    "    mape = np.mean(ape)\n",
    "\n",
    "    return mape\n",
    "\n",
    "def mean_squared_error(values_observed, values_predicted):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Función auxiliar para calcular el error cuadrático medio.\n",
    "\n",
    "    Parámetros:\n",
    "\n",
    "        * values_observed: secuencia con los valores observados (True).\n",
    "        * values_predicted: secuencia con los valores predichos.\n",
    "\n",
    "    Return:\n",
    "\n",
    "        * mse: mean squared error.\n",
    "    '''\n",
    "\n",
    "    se = [(value_observed - value_predicted) ** 2 for value_observed, value_predicted in zip(values_observed, values_predicted)]\n",
    "    mse = np.mean(se)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Location:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase para cualquier punto del área de servicio que no es depot, customer o idlepoint.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * id: 'Service Area'.\n",
    "        * pos: coordenadas de posición.\n",
    "        * t_start_serving: tiempo en que el vehículo llega a la posición.\n",
    "        * t_leave: tiempo en que el vehículo deja la posición.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        self.id = 'Service Area'\n",
    "        self.pos = None\n",
    "        self.t_start_serving = None\n",
    "        self.t_leave = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Depot:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase para puntos del área de servicio que son depot.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * id: 'Depot'.\n",
    "        * pos: coordenadas de posición.\n",
    "        * t_start_serving: tiempo en que el vehículo llega a la posición.\n",
    "        * t_leave: tiempo en que el vehículo deja la posición.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        self.id = 'Depot'\n",
    "        self.pos = None\n",
    "        self.t_start_serving = None\n",
    "        self.t_leave = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase IdlePoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdlePoint:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase para puntos del área de servicio que son depot.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * id: id único para el idlepoint.\n",
    "        * pos: coordenadas de posición.\n",
    "        * t_start_serving: tiempo en que el vehículo llega a la posición.\n",
    "        * t_leave: tiempo en que el vehículo deja la posición.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        self.id = None\n",
    "        self.pos = None\n",
    "        self.t_start_serving = None\n",
    "        self.t_leave = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customer:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase que genera objetos customer.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * id: id único para el objeto customer en la realización.\n",
    "        * pos: coordenadas de posición.\n",
    "        * t_arrival: tiempo en que aparece el cliente.\n",
    "        * category: valor numérico que representa el nivel de importancia del cliente (debería ser igual o mayor a 1).\n",
    "        * t_confirmed: si se confirma el cliente este atributo indica el instante en que se confirmó.\n",
    "        * t_start_serving: si se confirma el cliente este atributo indica el instante en que se comenzó a atender.\n",
    "        * t_leave: si se confirma el cliente este atributo indica el instante en que se dejó su posición.\n",
    "        * status: puede ser 'not seen', 'confirmed' o 'rejected'.\n",
    "        * penalty: indica la penalización acumulada que se ha generado por atender al cliente fuera de su ventana de tiempo.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        self.id = None\n",
    "        self.pos = None\n",
    "        self.t_arrival = None\n",
    "        self.category = None\n",
    "        self.t_confirmed = None\n",
    "        self.t_start_serving = None\n",
    "        self.t_leave = None\n",
    "        self.status = None\n",
    "        self.penalty = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase que genera objetos vehicle.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * id: id único para el objeto vehicle.\n",
    "        * route: lista que representa el plan de ruta del vehículo. El primer objeto de la lista indica la posición actual del vehículo, mientras que el objeto final de la lista es el depot. \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        self.id = None\n",
    "        self.route = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase que genera la estructura básica del VRP a resolver y contiene los parámetros de la simulación.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * A_x: tamaño (en km) en la dirección x del área de servicio.\n",
    "        * A_y: tamaño (en km) en la dirección y del área de servicio.\n",
    "        * n_vehicles: número de vehículos.\n",
    "        * n_cust: número de clientes.\n",
    "        * dod: grado de dinamismo (degree of dinamism) del problema. Entendido como la proporción: Clientes tardíos / Clientes totales.\n",
    "        * t_max: límite de tiempo (en min) para la llegada de pedidos.\n",
    "        * t_service: tiempo de servicio (en min) para los clientes.\n",
    "        * t_window: ventana de tiempo (en min) para la atención satisfactoria del cliente.\n",
    "        * cust_categories: posibles categorías de cliente.\n",
    "        * penalty_factor: factor de penalización por atención fuera de ventana de tiempo.\n",
    "        * t_delta: tiempo máximo (en min) entre puntos de decisión. Si no llega un cliente en t_delta minutos se transita a un nuevo punto de decisión.\n",
    "        * vel_mean: velocidad promedio de los vehículos (en km/min).\n",
    "        * vel_std: desviación estándar de la velocidad de los vehículos (en km/min).\n",
    "        * idle_points_pos: coordenadas de los puntos de espera.\n",
    "        * stoc_travel_time: booleano que indica si se trata de un problema con tiempos de viaje estocásticos (True) o determinísticos (False).\n",
    "\n",
    "        * self.depot: objeto de la clase Depot que indica donde comienzan y terminan el día los vehículos.\n",
    "        * vehicles: lista de objetos vehicles.\n",
    "        * idle_points: lista de objetos idlepoint.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, A_x, A_y, n_vehicles, n_cust, dod, t_max, t_service, t_window, cust_categories, penalty_factor, t_delta, vel_mean, vel_std, idle_points_pos, stoc_travel_time=True):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * A_x: tamaño en la dirección x del área de servicio. Se ingresa en km. \n",
    "            * A_y: tamaño en la dirección y del área de servicio. Se ingresa en km. \n",
    "            * n_vehicles: número de vehículos.\n",
    "            * n_cust: número de clientes.\n",
    "            * dod: número entre 0 y 1 que indica el grado de dinamismo.\n",
    "            * t_max: límite de tiempo para la llegada de pedidos. Se ingresa en horas.\n",
    "            * t_service: tiempo de servicio para los clientes. Se ingresa en minutos.\n",
    "            * t_window: ventana de tiempo para la atención satisfactoria del cliente. Se ingresa en minutos.\n",
    "            * cust_categories: lista que contiene las posibles categorías de cliente. Las categorías son numéricas.\n",
    "            * penalty_factor: factor de penalización por atención fuera de ventana de tiempo. Entre más cercano a 1 más fuerte la penalización. Un factor de 15 es adecuado.\n",
    "            * t_delta: tiempo máximo entre puntos de decisión. Se ingresa en minutos.\n",
    "            * vel_mean: velocidad promedio de los vehículos. Se ingresa en km/h.\n",
    "            * vel_std: desviación estándar de la velocidad de los vehículos. Se ingresa en km/h.\n",
    "            * idle_points_pos: lista que contiene tuplas con las coordenadas de los puntos de espera.\n",
    "            * stoc_travel_time: True o False para tiempos de viaje estocásticos o determinísticos respectivamente.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "\n",
    "        # se definen los atributos de la clase\n",
    "        self.A_x = A_x\n",
    "        self.A_y = A_y\n",
    "        self.n_vehicles = n_vehicles\n",
    "        self.n_cust = n_cust\n",
    "        self.dod = dod\n",
    "        self.t_max = t_max * 60\n",
    "        self.t_service = t_service\n",
    "        self.t_window = t_window\n",
    "        self.cust_categories = cust_categories\n",
    "        self.penalty_factor = penalty_factor\n",
    "        self.t_delta = t_delta\n",
    "        self.vel_mean = vel_mean / 60\n",
    "        self.vel_std = vel_std / 60\n",
    "        self.idle_points_pos = idle_points_pos\n",
    "        self.stoc_travel_time = stoc_travel_time\n",
    "\n",
    "        # se crea el depot\n",
    "        self.depot = Depot()\n",
    "        self.depot.pos = (self.A_x/2, self.A_y/2)\n",
    "\n",
    "        # se crean los vehículos\n",
    "        self.vehicles = []\n",
    "        for i in range(self.n_vehicles):\n",
    "            vehicle = Vehicle()\n",
    "            vehicle.id = 'v' + str(i)\n",
    "            self.vehicles.append(vehicle)\n",
    "            \n",
    "        # se crean los idle points\n",
    "        self.idle_points = []\n",
    "        for i in range(len(self.idle_points_pos)):\n",
    "            idle_point = IdlePoint()\n",
    "            idle_point.id = 'i' + str(i)\n",
    "            idle_point.pos = self.idle_points_pos[i]\n",
    "            self.idle_points.append(idle_point)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase que genera objetos state. Estos objetos representan los estados que se generan en el sistema con la llegada de un cliente o con el paso de un tiempo determinado.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * vehicles: lista de los vehículos generados con Instance. Permite acceder al plan de ruta de los vehículos.\n",
    "        * random_cust: objeto customer que corresponde al cliente que llega aleatoriamente entre un estado y otro.\n",
    "        * t: instante de tiempo (en min) actual del episodio.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, instance):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase State.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * instance: objeto de la clase Instance.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None\n",
    "        '''\n",
    "\n",
    "        self.vehicles = instance.vehicles\n",
    "        self.random_cust = None\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    def initialState(self, realization, instance):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que define el estado inicial del problema.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * realization: lista de clientes de una realización aleatoria.\n",
    "            * instance: objeto de la clase Instance.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None\n",
    "        '''\n",
    "\n",
    "        # ningun cliente ha sido visto, por ende, su status es 'None', su penalty actual es 0 y no hay tiempos de inicio y fin de servicio aun\n",
    "        for cust in realization:\n",
    "            cust.t_confirmed = None \n",
    "            cust.t_start_serving = None\n",
    "            cust.t_leave = None\n",
    "            cust.status = None\n",
    "            cust.penalty = 0\n",
    "\n",
    "        # ningun idlepoint ha sido visitado por lo que no hay tiempos de inicio y final de servicio\n",
    "        for idlepoint in instance.idle_points:\n",
    "            idlepoint.t_start_serving = None\n",
    "            idlepoint.t_leave = None\n",
    "\n",
    "        # el depot tiene tiempo de inicio y fin de servicio igual a 0 porque es la posición inicial\n",
    "            instance.depot.t_start_serving = 0\n",
    "            instance.depot.t_leave = 0\n",
    "\n",
    "        # la ruta inicial de cada vehículo sólo contiene el depot como posición inicial y final\n",
    "        for vehicle in self.vehicles:\n",
    "            vehicle.route = [instance.depot, instance.depot]\n",
    "\n",
    "        # el primer random customer es el primer cliente que tiene t_arrival = 0\n",
    "        for cust in realization:\n",
    "            if cust.t_arrival == 0:\n",
    "                self.random_cust = cust\n",
    "                break\n",
    "\n",
    "        # tiempo inicial del problema\n",
    "        self.t = 0\n",
    "\n",
    "\n",
    "    def isTerminalState(self, instance):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que permite identificar un estado terminal.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * instance: objeto de la clase Instance.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * True o False dependiendo de si el estado es terminal o no.\n",
    "        '''\n",
    "\n",
    "        # si todos los vehículos tienen el depot como única localización en el plan de ruta entonces es el estado terminal\n",
    "        for vehicle in self.vehicles:\n",
    "            if vehicle.route != [instance.depot]:\n",
    "                return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase StateAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateAction(State):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase que genera objetos state-action, que corresponde a un estado luego de haber aplicado una acción. Sobre estos objetos se calcula el value-to-go.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * vehicles: lista de los vehículos generados con Instance. Permite acceder al plan de ruta de los vehiculos.\n",
    "        * random_cust: objeto customer que corresponde al cliente que llega aleatoriamente.\n",
    "        * t: instante de tiempo (en min) actual del episodio.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, state, action):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase StateAction.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * state: objeto de la clase State.\n",
    "            * action: lista de vehículos que contienen sus respectivos planes de ruta.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None\n",
    "        '''\n",
    "\n",
    "        self.state_deepcopy = copy.deepcopy(state)\n",
    "\n",
    "        # la clase tiene los mismos atributos que el objeto state que toma como parámetro\n",
    "        self.vehicles = [copy.deepcopy(vehicle) for vehicle in state.vehicles]        \n",
    "        self.random_cust = copy.deepcopy(state.random_cust)\n",
    "        self.t = state.t\n",
    "\n",
    "        # cambia la ruta de cada vehículo según la acción\n",
    "        for vehicle, route in zip(self.vehicles, action):\n",
    "            vehicle.route = route\n",
    "\n",
    "        # se cambia el status de random_cust a confirmed o a rejected dependiendo si se insertó o no\n",
    "        if self.random_cust is not None:\n",
    "            self.random_cust.status = 'rejected'\n",
    "            for route in action:\n",
    "                if self.random_cust.id in [cust.id for cust in route]:\n",
    "                    self.random_cust.status = 'confirmed'\n",
    "                    self.random_cust.t_confirmed = self.t\n",
    "                    break\n",
    "            if self.random_cust.status == 'confirmed':\n",
    "                for vehicle in self.vehicles:\n",
    "                    for cust in vehicle.route:\n",
    "                        if cust.id == self.random_cust.id:\n",
    "                            cust.status = 'confirmed'\n",
    "                            cust.t_confirmed = self.t\n",
    "\n",
    "                \n",
    "    def getFeatures(self, instance):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para obtener características del objeto state.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * instance: objeto de la clase Instance.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * features: lista que representa un vector de características del objeto state.\n",
    "        '''\n",
    "\n",
    "        # feature constante\n",
    "        c = [1]\n",
    "\n",
    "        # tiempo restante de pedidos\n",
    "        t_left = [instance.t_max - self.t]\n",
    "\n",
    "        # variable binaria para la inserción del cliente\n",
    "        insertion_binary = [0]\n",
    "        if self.random_cust is not None:\n",
    "            for vehicle in self.vehicles:\n",
    "                if self.random_cust.id in [cust.id for cust in vehicle.route]:\n",
    "                    insertion_binary = [1]\n",
    "                    break\n",
    "\n",
    "        # costo (en distancia) de la inserción\n",
    "        route_len_pre, route_len_post = 0, 0\n",
    "        for vehicle_pre, vehicle_post in zip(self.state_deepcopy.vehicles, self.vehicles):\n",
    "            route_len_pre += euclideanDistance(vehicle_pre.route)\n",
    "            route_len_post += euclideanDistance(vehicle_post.route)\n",
    "        insertion_cost = [route_len_post - route_len_pre]\n",
    "\n",
    "        # número de clientes por atender\n",
    "        n_cust_pend = 0\n",
    "        for vehicle in self.vehicles:\n",
    "            n_cust_pend += sum(isinstance(cust, Customer) for cust in vehicle.route)\n",
    "        n_cust_pend = [n_cust_pend]\n",
    "\n",
    "        # desviación del largo (en número de localizaciones) de los planes de ruta\n",
    "        route_nlen = [len(vehicle.route) for vehicle in self.vehicles]\n",
    "        std_route_nlen = [np.std(route_nlen)]\n",
    "\n",
    "        # promedio de la raiz cuadrada del largo (en número de localizaciones) de los planes de ruta\n",
    "        sqrt_route_nlen = [np.sqrt(len(vehicle.route)) for vehicle in self.vehicles]\n",
    "        mean_sqrt_route_nlen = [np.mean(sqrt_route_nlen)]\n",
    "\n",
    "        # desviación de la raiz del largo (en número de localizaciones) de los planes de ruta\n",
    "        std_sqrt_route_nlen = [np.std(sqrt_route_nlen)]\n",
    "\n",
    "        # razón entre tiempo restante y el número de clientes por atender\n",
    "        t_per_n_cust_pend = [t_left[0] / max(1, n_cust_pend[0])]\n",
    "\n",
    "        # promedio del largo (en distancia) del plan de ruta\n",
    "        route_len = [sum(euclideanDistance([vehicle.route[i], vehicle.route[i+1]]) for i in range(len(vehicle.route)-1)) for vehicle in self.vehicles if len(vehicle.route) > 1]\n",
    "        mean_route_len = [np.mean(route_len)]\n",
    "\n",
    "        # desviación del largo (en distancia) de los planes de ruta\n",
    "        std_route_len = [np.std(route_len)]\n",
    "\n",
    "        # producto entre el tiempo restante y el largo promedio de las rutas\n",
    "        t_mean_route_len = [t_left[0] * mean_route_len[0]]\n",
    "\n",
    "        # promedio de la distancia entre los vehículos\n",
    "        vehicles_dist = []\n",
    "        for vehicle_pair in itertools.combinations(self.vehicles, 2):\n",
    "            dist = euclideanDistance([vehicle_pair[0].route[0], vehicle_pair[1].route[0]])\n",
    "            vehicles_dist.append(dist)\n",
    "        mean_vehicles_dist = [np.mean(vehicles_dist)]\n",
    "\n",
    "        # promedio de la desviación de la distancia entre todos las localizaciones de las rutas\n",
    "        std_vehicles_loc_dist = []\n",
    "        for vehicle in self.vehicles:\n",
    "            loc_dist = [euclideanDistance([a, b]) for a, b in itertools.combinations(vehicle.route, 2)]\n",
    "            std_vehicles_loc_dist.append(np.std(loc_dist))\n",
    "        mean_std_loc_dist = [np.mean(std_vehicles_loc_dist)]\n",
    "\n",
    "        # promedio al cuadrado de la desviación de la distancia entre todos las localizaciones de las rutas\n",
    "        squared_mean_std_loc_dist = [mean_std_loc_dist[0] ** 2]\n",
    "\n",
    "        # promedio de la distancia máxima entre dos puntos en las rutas\n",
    "        vehicles_max_dist = []\n",
    "        for vehicle in self.vehicles:\n",
    "            max_dist = 0\n",
    "            for i, j in itertools.combinations(range(len(vehicle.route)), 2):\n",
    "                dist = euclideanDistance([vehicle.route[i], vehicle.route[j]])\n",
    "                if dist > max_dist:\n",
    "                    max_dist = dist\n",
    "        vehicles_max_dist.append(max_dist)\n",
    "        mean_max_dist = [np.mean(vehicles_max_dist)]\n",
    "        \n",
    "        # promedio al cuadrado de la distancia máxima entre dos puntos en las rutas\n",
    "        squared_mean_max_dist = [mean_max_dist[0] ** 2]\n",
    "\n",
    "        # std del factor de ocupación\n",
    "        vehicles_of = [np.sum([isinstance(loc, Customer) for loc in vehicle.route]) / max(1, n_cust_pend[0]) for vehicle in self.vehicles]\n",
    "        std_vehicles_of = [np.std(vehicles_of)]\n",
    "\n",
    "        # promedio del penalty en el sistema\n",
    "        vehicles_penalties = []\n",
    "        for vehicle in self.vehicles:\n",
    "            penalty = 0\n",
    "            for cust in vehicle.route:\n",
    "                if cust.t_start_serving is None:\n",
    "                    if isinstance(cust, Customer) and self.t >= cust.t_confirmed + instance.t_window:\n",
    "                        extra_time = self.t - (cust.t_confirmed + instance.t_window)\n",
    "                        penalty += round(log(extra_time + 1, instance.penalty_factor), 2)\n",
    "                elif cust.t_start_serving is not None:\n",
    "                    if isinstance(cust, Customer) and cust.t_start_serving >= cust.t_confirmed + instance.t_window:\n",
    "                        extra_time = cust.t_start_serving - (cust.t_confirmed + instance.t_window)\n",
    "                        penalty += round(log(extra_time + 1, instance.penalty_factor), 2)\n",
    "            vehicles_penalties.append(penalty)\n",
    "        mean_penalty = [np.mean(vehicles_penalties)]\n",
    "\n",
    "        # std del penalty en el sistema\n",
    "        std_penalty = [np.std(vehicles_penalties)]\n",
    "\n",
    "        # promedio de atraso en el sistema\n",
    "        vehicles_delays = []\n",
    "        for vehicle in self.vehicles:\n",
    "            delay = 0\n",
    "            for cust in vehicle.route:\n",
    "                if cust.t_start_serving is None:\n",
    "                    if isinstance(cust, Customer) and self.t >= cust.t_confirmed + instance.t_window:\n",
    "                        extra_time = self.t - (cust.t_confirmed + instance.t_window)\n",
    "                        delay += extra_time\n",
    "                elif cust.t_start_serving is not None:\n",
    "                    if isinstance(cust, Customer) and cust.t_start_serving >= cust.t_confirmed + instance.t_window:\n",
    "                        extra_time = cust.t_start_serving - (cust.t_confirmed + instance.t_window)\n",
    "                        delay += extra_time\n",
    "            vehicles_delays.append(delay)\n",
    "        mean_delay = [np.mean(vehicles_delays)]\n",
    "\n",
    "        # desviación del atraso en el sistema\n",
    "        std_delay = [np.std(vehicles_delays)]\n",
    "\n",
    "        # tiempo restante por promedio del atraso\n",
    "        t_mean_delay = [t_left[0] * mean_delay[0]]\n",
    "\n",
    "        # eficiencia promedio de los planes de ruta\n",
    "        route_efficiency = [length / nlength for length, nlength in zip(route_len, route_nlen)]\n",
    "        mean_route_efficency = [np.mean(route_efficiency)]\n",
    "        \n",
    "        # se crea el vector de features\n",
    "        features = np.array([c + \n",
    "                             t_left +\n",
    "                             insertion_binary +\n",
    "                             insertion_cost +\n",
    "                             n_cust_pend +\n",
    "                             std_route_nlen +\n",
    "                             mean_sqrt_route_nlen +\n",
    "                             std_sqrt_route_nlen +\n",
    "                             t_per_n_cust_pend +\n",
    "                             mean_route_len +\n",
    "                             std_route_len +\n",
    "                             t_mean_route_len +\n",
    "                             mean_vehicles_dist +\n",
    "                             mean_std_loc_dist +\n",
    "                             squared_mean_std_loc_dist +\n",
    "                             mean_max_dist +\n",
    "                             squared_mean_max_dist +\n",
    "                             std_vehicles_of +\n",
    "                             mean_penalty +\n",
    "                             std_penalty +\n",
    "                             mean_delay +\n",
    "                             std_delay +\n",
    "                             t_mean_delay +\n",
    "                             mean_route_efficency]).T\n",
    "\n",
    "        # # ESTE ES EL MEJOR PARA INSTANCIA SENCILLA\n",
    "        # # se crea el vector de features\n",
    "        # features = np.array([c +\n",
    "        #                      t_left +\n",
    "        #                      insertion_binary +\n",
    "        #                      insertion_cost +\n",
    "        #                      n_cust_pend +\n",
    "        #                      mean_max_dist +\n",
    "        #                      std_vehicles_of +\n",
    "        #                      mean_delay]).T\n",
    "        \n",
    "        # # ESTE ES EL MEJOR PARA INSTANCIA COMPLEJA FUNCIONA MEJOR CON LAMBDA = 1e10\n",
    "        # # se crea el vector de features\n",
    "        # features = np.array([c +\n",
    "        #                      t_left +\n",
    "        #                      insertion_binary +\n",
    "        #                      insertion_cost +\n",
    "        #                      n_cust_pend +\n",
    "        #                      t_per_n_cust_pend +\n",
    "        #                      mean_vehicles_dist +\n",
    "        #                      mean_std_loc_dist +\n",
    "        #                      mean_max_dist +\n",
    "        #                      mean_penalty]).T\n",
    "\n",
    "        return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Esta clase contiene la estructura del modelo; genera objetos que contienen el MDP.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * None.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        pass\n",
    "    \n",
    "\n",
    "    def computeActions(self, state, instance):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para determinar las acciones posibles en un estado.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * state: objeto de la clase State.\n",
    "            * instance: objeto de la clase Instance.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * actions: lista que contiene todas las acciones posibles para state.\n",
    "        '''\n",
    "\n",
    "        # Se definen los vehículos más cercanos al posible random_cust\n",
    "        if state.random_cust is not None:\n",
    "            distances = []\n",
    "            for vehicle in state.vehicles:\n",
    "                dist = euclideanDistance([vehicle.route[0], state.random_cust])\n",
    "                distances.append((vehicle, dist))\n",
    "            closest = heapq.nsmallest(4, distances, key=lambda x: x[1])\n",
    "            closest_vehicles = [vehicle for vehicle, _ in closest]\n",
    "            \n",
    "        # se obtienen las acciones para cada vehículo\n",
    "        actions_per_vehicle = []\n",
    "        for vehicle in state.vehicles:\n",
    "            actions_one_vehicle = []\n",
    "\n",
    "            # 1. insertar cliente en alguna ruta de vehículos cercanos (y posiblemente eliminar idlepoint)\n",
    "            if state.random_cust is not None and vehicle in closest_vehicles:\n",
    "                # se calcula el largo de las diferentes rutas que se generan por las distintas inserciones\n",
    "                routes_len = []\n",
    "                # si el vehículo tiene como siguiente posición un idlepoint, este se puede eliminar y se inserta al cliente\n",
    "                if isinstance(vehicle.route[1], IdlePoint):\n",
    "                    route_copy = copy.copy(vehicle.route)\n",
    "                    route_copy[1] = state.random_cust\n",
    "                    length = euclideanDistance(route_copy)\n",
    "                    routes_len.append((route_copy, length))\n",
    "                # si no, se considera la inserción del cliente en cualquier punto de la ruta\n",
    "                else:\n",
    "                    for i in range(1, len(vehicle.route)):\n",
    "                        route_copy = copy.copy(vehicle.route)\n",
    "                        route_copy.insert(i, state.random_cust)\n",
    "                        length = euclideanDistance(route_copy)\n",
    "                        routes_len.append((route_copy, length))\n",
    "                # sólo se consideran las mejores inserciones como posibles acciones\n",
    "                best_insertions = heapq.nsmallest(6, routes_len, key=lambda x: x[1])\n",
    "                best_insertions_routes = [route for route, _ in best_insertions]\n",
    "                for route in best_insertions_routes:\n",
    "                    actions_one_vehicle.append(route)\n",
    "\n",
    "            # # 2. ir a un idlepoint. Condiciones necesarias: no hay clientes por empezar a atender ni idlepoints en la ruta, y el idlepoint se encuentra los suficientmente lejos de la posición del vehículo\n",
    "            # if not any(isinstance(loc, (Customer, IdlePoint)) for loc in vehicle.route[1:]):\n",
    "            #     for idlepoint in instance.idle_points:\n",
    "            #         dist = euclideanDistance([vehicle.route[0], idlepoint])\n",
    "            #         min_dist_idlepoint = np.hypot(instance.A_x, instance.A_y) / 3\n",
    "            #         if dist >= min_dist_idlepoint:\n",
    "            #             route_copy = copy.copy(vehicle.route)\n",
    "            #             route_copy.insert(1, idlepoint)\n",
    "            #             actions_one_vehicle.append(route_copy)\n",
    "\n",
    "            # 3. seguir con la ruta eliminando el idlepoint. Condiciones necesarias: vehicle.route[0] no es service area y vehicle.route[1] es idlepoint\n",
    "            if not isinstance(vehicle.route[0], Location) and isinstance(vehicle.route[1], IdlePoint):\n",
    "                route_copy = copy.copy(vehicle.route)\n",
    "                route_copy.pop(1)\n",
    "                actions_one_vehicle.append(route_copy)\n",
    "\n",
    "            # 4. seguir con la ruta. Condiciones necesarias: vehicle.route no es del tipo ['service area', 'Depot'] porque no se puede esperar en \"service area\"\n",
    "            if not (isinstance(vehicle.route[0], Location) and isinstance(vehicle.route[1], Depot)):\n",
    "                actions_one_vehicle.append(vehicle.route)\n",
    "\n",
    "            # se guardan las acciones del vehículo\n",
    "            actions_per_vehicle.append(actions_one_vehicle)\n",
    "\n",
    "        # se crean las acciones, donde por cada acción individual de un vehículo, los demás vehículos no toman acción de movimiento\n",
    "        actions = []\n",
    "        for i, actions_one_vehicle in enumerate(actions_per_vehicle):\n",
    "            for route in actions_one_vehicle:\n",
    "                action = [vehicle.route for vehicle in state.vehicles]\n",
    "                if state.random_cust is None or state.random_cust in route or (state.random_cust not in route and state.random_cust.t_arrival != 0):\n",
    "                    action[i] = route\n",
    "                    actions.append(action)\n",
    "\n",
    "        # se elimina la repetición de la accion donde todos los vehículos se mantienen con la misma ruta\n",
    "        dic = {}\n",
    "        for action in actions:\n",
    "            key = str(action)\n",
    "            dic[key] = action\n",
    "        actions = list(dic.values())\n",
    "\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def transition(self, state, action, realization, instance):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que determina la transición aleatoria entre un estado y otro.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * state: objeto de la clase State.\n",
    "            * action: lista de vehículos que contienen sus respectivos planes de ruta.\n",
    "            * realization: lista de clientes de una realización aleatoria.\n",
    "            * instance: objeto de la clase Instance.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * state: estado actualizado luego de la transición. Corresponde a un objeto de la clase State.\n",
    "            * reward: reward total (ganancias - penalizaciones) percibido en la transición.\n",
    "        '''\n",
    "                    \n",
    "        # actualización del random customer\n",
    "        random_cust_before_transition = state.random_cust\n",
    "        state.random_cust = None\n",
    "        for cust in realization:\n",
    "            if state.t <= cust.t_arrival <= state.t + instance.t_delta and cust.status is None:\n",
    "                state.random_cust = cust\n",
    "                break\n",
    "\n",
    "        # actualización del tiempo del estado: cuando llega un cliente o pasados t_delta minutos si no ha llegado ningun cliente\n",
    "        t_before_transition = state.t\n",
    "        if state.random_cust is None:\n",
    "            state.t += instance.t_delta\n",
    "            # si luego de t_delta sin llegada de clientes se supera el t_max, significa que se deben atender los clientes pendientes y volver al depot\n",
    "            if state.t > instance.t_max:\n",
    "                state.t = float('inf')\n",
    "        else:\n",
    "            state.t = state.random_cust.t_arrival\n",
    "\n",
    "        # actualización del plan de ruta de cada vehículo\n",
    "        for vehicle, route in zip(state.vehicles, action):\n",
    "\n",
    "            # si se superó t_max y la ruta tiene un idlepoint por visitar, este se elimina de la ruta y se va directo al depot\n",
    "            if state.t == float('inf'):\n",
    "                if isinstance(route[1], IdlePoint):\n",
    "                    route.remove(route[1])\n",
    "\n",
    "            # cuando se supera el t_max se atienden los clientes restantes y se vuelve al depot, porque no llegan clientes después de t_max\n",
    "            if state.t > instance.t_max:\n",
    "                range_sup = len(route)\n",
    "            # cuando aun no se supera t_max se llega sólo hasta el último cliente y se espera en esa posición, no se llega hasta el depot\n",
    "            else:\n",
    "                range_sup = len(route)-1\n",
    "            \n",
    "            # se itera sobre cada loc de la ruta\n",
    "            for i in range(range_sup):\n",
    "                # se samplea un velocidad para el tramo\n",
    "                vel = sampleVel(instance)\n",
    "                # si se está en el depot, el tiempo en que se deja la posición es t del estado previo a la transición\n",
    "                if isinstance(route[i], Depot):\n",
    "                    route[i].t_leave = t_before_transition\n",
    "                # se define la variable time como el tiempo en que se deja la posición i\n",
    "                time = route[i].t_leave\n",
    "                # si time es menor a t y no es el último cliente de la ruta, se calcula la distancia entre la loc i e i+1\n",
    "                if time < state.t and i != range_sup-1:\n",
    "                    time += round(euclideanDistance([route[i], route[i+1]]) / vel, 1)\n",
    "                    # si time considerando la duración del viaje entre i e i+1 es menor a t entonces se alcanza a llegar a i+1\n",
    "                    if time < state.t:\n",
    "                        # se define el tiempo en que se comienza a atender y se deja i+1\n",
    "                        route[i+1].t_start_serving = time\n",
    "                        # se define el tiempo en que se deja la posición\n",
    "                        if isinstance(route[i+1], Customer):\n",
    "                            route[i+1].t_leave = route[i+1].t_start_serving + instance.t_service\n",
    "                        else:\n",
    "                            route[i+1].t_leave = route[i+1].t_start_serving\n",
    "                    # si time considerando la duración del viaje entre i e i+1 no es menor a t entonces el vehículo queda en una localización entre i e i+1\n",
    "                    else:\n",
    "                        t_travel = state.t - route[i].t_leave\n",
    "                        cos = (route[i+1].pos[0] - route[i].pos[0]) / euclideanDistance([route[i], route[i+1]])\n",
    "                        sen = (route[i+1].pos[1] - route[i].pos[1]) / euclideanDistance([route[i], route[i+1]])\n",
    "                        vel_x = vel * cos\n",
    "                        vel_y = vel * sen\n",
    "                        pos_x = round(route[i].pos[0] + vel_x * t_travel, 2)\n",
    "                        pos_y = round(route[i].pos[1] + vel_y * t_travel, 2)\n",
    "                        vehicle_loc = Location()\n",
    "                        vehicle_loc.pos = (pos_x, pos_y)\n",
    "                        vehicle_loc.t_start_serving = state.t\n",
    "                        vehicle_loc.t_leave = vehicle_loc.t_start_serving\n",
    "                        vehicle.route = [vehicle_loc] + route[i+1:]\n",
    "                        break\n",
    "\n",
    "                # si time es mayor o igual que t, el vehículo se encuentra en el punto i de la ruta\n",
    "                elif time >= state.t:\n",
    "                    vehicle.route = route[i:]\n",
    "                    break\n",
    "                \n",
    "                # si se está en el último punto de la ruta, el vehículo se encuentra en el punto i de la ruta\n",
    "                elif i == range_sup-1:\n",
    "                    route[i].t_leave = state.t\n",
    "                    vehicle.route = route[i:]\n",
    "                    break\n",
    "\n",
    "        # se calcula el reward de la transición\n",
    "        # en primer lugar se considera la ganancia de haber aceptado al cliente o no   \n",
    "        reward = 0\n",
    "        for route in action:\n",
    "            if random_cust_before_transition in route:\n",
    "                reward += random_cust_before_transition.category\n",
    "                break\n",
    "        # en segundo lugar se calcula la penalización por clientes atendidos fuera de su ventana de tiempo\n",
    "        for route in action:\n",
    "            for cust in route:\n",
    "                extra_time = 0\n",
    "                if cust.t_start_serving is None:\n",
    "                    if isinstance(cust, Customer) and state.t > cust.t_confirmed + instance.t_window:\n",
    "                        extra_time = state.t - (cust.t_confirmed + instance.t_window)\n",
    "                elif cust.t_start_serving is not None:\n",
    "                    if isinstance(cust, Customer) and cust.t_start_serving > cust.t_confirmed + instance.t_window:\n",
    "                        extra_time = cust.t_start_serving - (cust.t_confirmed + instance.t_window)\n",
    "                if extra_time > 0:\n",
    "                    penalty = round(log(extra_time + 1, instance.penalty_factor), 2) - cust.penalty\n",
    "                    reward -= penalty\n",
    "                    cust.penalty += penalty\n",
    "\n",
    "        return state, reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ValueFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Clase que crea un objeto value function, que representa la aproximación lineal de la value function.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * weights: pesos de la regresión asociados a los features.\n",
    "        * n_features: número de features que se extraen de un objeto stateaction para obtener la aproximación lineal.\n",
    "        * B: matriz para la actualización de los pesos de la regresión. Este atributo sólo se crea cuando se utiliza el algoritmo RLS para actualizar la value function.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, initial_weights=None):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * initial_weights: pesos iniciales de la regresión. Si no se ingresa un valor, se considera un vector de ceros.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        # se calcula la cantidad de features\n",
    "        self.n_features = 24\n",
    "        # se define el vector de pesos inicial\n",
    "        if initial_weights is None:\n",
    "            self.weights = np.zeros(self.n_features).reshape(1, -1).T\n",
    "        else:\n",
    "            self.weights = initial_weights.reshape(1, -1).T\n",
    "\n",
    "\n",
    "    def initializeRecursiveLeastSquares(self, lambd):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para la inicialización de la matriz de actualización B de Recursive Least Squares para el entrenamiento de la aproximación lineal de la value function.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * lambd: parámetro de penalización (ridge) para la aproximación lineal de la value function.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "\n",
    "        # inicialización de la matriz B para la actualización de los weights\n",
    "        self.B = lambd * np.identity(self.n_features)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que entrega el value-to-go aproximado para un stateaction.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * features: features de un stateaction.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * value_pred: value predicho por la aproximación lineal de la value function.\n",
    "        '''\n",
    "\n",
    "        # se obtiene el value aproximado a partir de los parámetros actuales de la value function\n",
    "        value_pred = np.dot(self.weights.T, features)[0][0]\n",
    "        \n",
    "        return value_pred\n",
    "\n",
    "\n",
    "    def updateWeights(self, features, value_predicted, value_observed):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que actualiza los pesos de la regresión.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * features: features observados para un stateaction.\n",
    "            * value_predicted: value-to-go predicho para el stateaction a partir de los features.\n",
    "            * value_observed: value-to-go observado para el stateaction.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        # se actualiza gamma\n",
    "        gamma = 1 + features.T @ self.B @ features\n",
    "        # se calcula el error del value\n",
    "        error = value_predicted - value_observed\n",
    "        # se actualiza la matriz H\n",
    "        H = (1 / gamma) * self.B\n",
    "        # se actualizan los pesos\n",
    "        self.weights = self.weights - (H @ features) * error\n",
    "        # se actualiza la matriz B\n",
    "        self.B = self.B - (1 / gamma) * (self.B @ features @ features.T @ self.B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superclase Algorithm y subclases de algoritmos derivados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm:\n",
    "    \n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Super clase que crea un objeto algorithm, que permite crear las realizaciones aleatorias.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * instance: objeto de la clase Instance que contiene las características del problema.\n",
    "        * process: objeto de la clase Process que contiene el MDP.\n",
    "    ''' \n",
    "\n",
    "    def __init__(self, instance, process):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método constructor de la clase.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * instance: objeto de la clase Instance que contiene las características del problema.\n",
    "            * process: objeto de la clase Process que contiene el MDP.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "\n",
    "        self.instance = instance\n",
    "        self.process = process\n",
    "\n",
    "    \n",
    "    def simulateRealizations(self, N, simulation_seed):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para crear simular realizaciones aleatorias.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * N: n° de realizaciones a simular.\n",
    "            * simulation_seed: seed para replicar las realizaciones generadas.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * simulated_realizations: realizaciones aleatorias.\n",
    "        '''\n",
    "\n",
    "        random.seed(simulation_seed)\n",
    "        realizations = []\n",
    "        for _ in range(N):\n",
    "            n_cust = self.instance.n_cust\n",
    "            realization = []\n",
    "            # se crean los early request customers (t_arrival = 0)\n",
    "            for j in range(ceil((1-self.instance.dod)*n_cust)):\n",
    "                customer = Customer()\n",
    "                customer.id = 'C' + str(j)\n",
    "                customer.pos = (round(random.uniform(0, self.instance.A_x), 2), round(random.uniform(0, self.instance.A_y), 2))\n",
    "                customer.category = random.choice(self.instance.cust_categories)\n",
    "                customer.t_arrival = 0\n",
    "                realization.append(customer)\n",
    "            # se crean los late request customers (t_arrival > 0)\n",
    "            for j in range(ceil((1-self.instance.dod)*n_cust), n_cust):\n",
    "                customer = Customer()\n",
    "                customer.id = 'C' + str(j)\n",
    "                customer.pos = (round(random.uniform(0, self.instance.A_x), 2), round(random.uniform(0, self.instance.A_y), 2))\n",
    "                customer.t_arrival = random.randint(1, self.instance.t_max)\n",
    "                customer.category = random.choice(self.instance.cust_categories)\n",
    "                realization.append(customer)\n",
    "            # se ordena la realización por tiempo de llegada de los clientes\n",
    "            realization = sorted(realization, key=lambda cust : cust.t_arrival)\n",
    "            realizations.append(realization)\n",
    "        simulated_realizations = realizations\n",
    "\n",
    "        return simulated_realizations\n",
    "\n",
    "\n",
    "    def simulateTrainRealizations(self, N, simulation_seed=None):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para crear las realizaciones de train.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * N: n° de realizaciones a simular.\n",
    "            * simulation_seed: seed para replicar las realizaciones generadas.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "        \n",
    "        self.train_realizations = self.simulateRealizations(N, simulation_seed)\n",
    "        \n",
    "\n",
    "    def simulateTestRealizations(self, N, simulation_seed=None):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para crear las realizaciones de train.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * N: n° de realizaciones a simular.\n",
    "            * simulation_seed: seed para replicar las realizaciones generadas.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * None.\n",
    "        '''\n",
    "\n",
    "        self.test_realizations = self.simulateRealizations(N, simulation_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheapestInsertion(Algorithm):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Subclase que crea un objeto del algortimo Cheapest Insertion. Esta clase contiene la forma una forma de resolver el VRP.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * instance: objeto de la clase Instance que contiene las características del problema.\n",
    "        * process: objeto de la clase Process que contiene el MDP.\n",
    "        * train_realizations: conjunto de realizaciones para entrenar el algoritmo.\n",
    "        * test_realizations: conjunto de realizaciones para testear el algoritmo.\n",
    "    ''' \n",
    "\n",
    "    def takeAction(self, state, actions):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para que selecciona una acción en cierto estado a partir de un conjunto de acciones.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * state: objeto de la clase State.\n",
    "            * actions: lista que contiene todas las acciones posibles para state.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * min_len_action: entrega la acción que tiene el menor costo en distancia recorrida. Corresponde a una lista que contiene los planes de ruta de cada vehículo.\n",
    "        '''\n",
    "\n",
    "        # se define cuál es el vehículo más cercano\n",
    "        if state.random_cust is not None:\n",
    "            min_dist = float('inf')\n",
    "            for i, vehicle in enumerate(state.vehicles):\n",
    "                dist = euclideanDistance([vehicle.route[0], state.random_cust])\n",
    "                if dist < min_dist:\n",
    "                    closest_vehicle_index = i\n",
    "                    min_dist = dist\n",
    "                            \n",
    "        # se calcula el costo de la acción con la inserción en el vehículo más cercano y se obtiene el mínimo\n",
    "        min_len = float('inf')\n",
    "        for action in actions:\n",
    "            # se consideran sólo las acciones donde se inserta el cliente en el vehículo más cercano\n",
    "            if (state.random_cust is not None and state.random_cust in action[closest_vehicle_index]) or state.random_cust is None:\n",
    "                action_len = 0\n",
    "                for route in action:\n",
    "                    # se cuantifica la distancia total de la ruta del vehículo más cercano\n",
    "                    action_len += euclideanDistance(route)\n",
    "                if action_len < min_len:\n",
    "                    min_len = action_len\n",
    "                    min_len_action = action\n",
    "                    \n",
    "        # se cambia el status de random_cust (del estado justo antes de la transición) a confirmed o a rejected dependiendo si se insertó o no\n",
    "        if state.random_cust is not None:\n",
    "            state.random_cust.status = 'rejected'\n",
    "            state.random_cust.t_confirmed = None\n",
    "            for route in min_len_action:\n",
    "                # si se insertó el cliente en la acción su status es 'confirmed'\n",
    "                if state.random_cust in route:\n",
    "                    state.random_cust.status = 'confirmed'\n",
    "                    state.random_cust.t_confirmed = state.t\n",
    "                    break\n",
    "\n",
    "        return min_len_action\n",
    "    \n",
    "\n",
    "    def test(self):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que aplica la política de decisión CI a una realizacion.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * None.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * test_reward: rewards obtenidos en las realizaciones test aplicando la política CI.\n",
    "            * penalties: valor total de penalización.\n",
    "            * n_penalties: número de clientes atendidos con retraso.\n",
    "            * n_rejects: número de clientes rechazados.\n",
    "        '''\n",
    "\n",
    "        test_rewards = []\n",
    "        penalties = []\n",
    "        n_penalties = []\n",
    "        n_rejects = []\n",
    "\n",
    "        for realization in copy.deepcopy(self.test_realizations):\n",
    "\n",
    "            # se crea una instancia de state\n",
    "            state = State(self.instance)\n",
    "            # se crea variable para calcular el reward del episodio\n",
    "            episode_reward = 0\n",
    "            # se define el estado inicial\n",
    "            state.initialState(realization, self.instance)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # si se está en el estado terminal termina el problema\n",
    "                if state.isTerminalState(self.instance):\n",
    "                    break\n",
    "                # se buscan las acciones posibles en el estado\n",
    "                actions = self.process.computeActions(state, self.instance)\n",
    "                # se toma la decisión devolviendo la accion\n",
    "                action = self.takeAction(state, actions)\n",
    "                # a partir de la acción y el estado se genera la transición al siguiente estado\n",
    "                state, reward = self.process.transition(state, action, realization, self.instance)\n",
    "                # se actualiza el reward total\n",
    "                episode_reward += reward\n",
    "\n",
    "            # se guarda el reward total\n",
    "            test_rewards.append(episode_reward)\n",
    "            # se guarda el penalty de la realización\n",
    "            penalties.append(sum(cust.penalty for cust in realization))\n",
    "            # se guarda la cantidad de pedidos atrasados\n",
    "            n_penalties.append(sum(cust.penalty > 0 for cust in realization))\n",
    "            # se guarda la cantidad de rechazos de la realización\n",
    "            n_rejects.append(sum(cust.status == 'rejected' for cust in realization))\n",
    "        \n",
    "        return test_rewards, penalties, n_penalties, n_rejects\n",
    "    \n",
    "\n",
    "    def train(self, ridge_penalty):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que aplica el algoritmo On policy Monte Carlo control (AVI) para encontrar una aproximación de la value function.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * ridge_penalty: coeficiente de penalización ridge. \n",
    "\n",
    "        Return:\n",
    "\n",
    "            * lr: objeto de la clase Ridge de Sklearn que contiene los pesos de la aproximación lineal entrenada.\n",
    "            * train_rewards: rewards obtenidos en las realizaciones de train.\n",
    "            * penalties: valor total de penalización.\n",
    "            * n_penalties: número de clientes atendidos con retraso.\n",
    "            * n_rejects: número de clientes rechazados.\n",
    "            * train_mape: mape obtenido en train.\n",
    "            * train_mse: mse obtenido en train.\n",
    "        '''\n",
    "\n",
    "        all_values_obs = []\n",
    "        all_features_obs = []\n",
    "        train_rewards = []\n",
    "        penalties = []\n",
    "        n_penalties = []\n",
    "        n_rejects = []\n",
    "\n",
    "        for realization in copy.deepcopy(self.train_realizations):\n",
    "            # se crea una instancia de state\n",
    "            state = State(self.instance)\n",
    "            # lista para guardar los rewards recibidos durante la realizacion\n",
    "            realization_rewards = []\n",
    "            # lista para guardar values observados (o returns) recibidos durante la realizacion\n",
    "            realization_values_obs = []\n",
    "            # lista para guardar los features extraidos de los state_action\n",
    "            realization_features = []\n",
    "            # se define el estado inicial\n",
    "            state.initialState(realization, self.instance)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # si se está en el estado terminal termina el problema\n",
    "                if state.isTerminalState(self.instance):\n",
    "                    break\n",
    "                # se definen las acciones\n",
    "                actions = self.process.computeActions(state, self.instance)\n",
    "                # se aplica take action sobre el conjunto de acciones\n",
    "                action = self.takeAction(state, actions)\n",
    "                # se crea un objeto state_action\n",
    "                state_action = StateAction(state, action)\n",
    "                # se extraen las features del state_action\n",
    "                features = state_action.getFeatures(self.instance)\n",
    "                features = features.T.tolist()[0]\n",
    "                features = features[1:]\n",
    "                # se almacenan los features en la lista\n",
    "                realization_features.append(features)\n",
    "                # se aplica la transición aleatoria creando un estado nuevo y un reward\n",
    "                state, reward = self.process.transition(state, action, realization, self.instance)\n",
    "                # se guarda el reward observado en la primera posición de lista\n",
    "                realization_rewards.append(reward)\n",
    "\n",
    "            # a continuación se calcula y almacena el value observado para cada state_action visitado durante la realizacion\n",
    "            realization_values_obs = list(itertools.accumulate(realization_rewards[::-1]))[::-1]\n",
    "            # se agregan los values observados en la realización a la lista de values observados\n",
    "            all_values_obs += realization_values_obs\n",
    "            # se agregan los features observados en la realización a la lista de features observados\n",
    "            all_features_obs += realization_features\n",
    "            # se guarda el reward del episodio\n",
    "            train_rewards.append(np.sum(realization_rewards))\n",
    "            # se guarda el penalty de la realización\n",
    "            penalties.append(sum(cust.penalty for cust in realization))\n",
    "            # se guarda la cantidad de pedidos atrasados\n",
    "            n_penalties.append(sum(cust.penalty > 0 for cust in realization))\n",
    "            # se guarda la cantidad de rechazos de la realización\n",
    "            n_rejects.append(sum(cust.status == 'rejected' for cust in realization))\n",
    "            \n",
    "        # a partir de los values y features observados en todas las realizaciones, se entrena una regresión lineal de sklearn\n",
    "        lr = Ridge(alpha=ridge_penalty)\n",
    "        lr.fit(all_features_obs, all_values_obs)\n",
    "        values_pred = lr.predict(all_features_obs)\n",
    "\n",
    "        # se calculan los errores de entrenamiento\n",
    "        train_mape = round(mean_absolute_percentage_error(all_values_obs, values_pred), 2)\n",
    "        train_mse = round(mean_squared_error(all_values_obs, values_pred), 2)\n",
    "\n",
    "        return lr, train_rewards, penalties, n_penalties, n_rejects, train_mape, train_mse\n",
    "    \n",
    "\n",
    "    def takeActionLr(self, state, actions, lr):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para que selecciona una acción en cierto estado a partir de un conjunto de acciones. Para esto, se aproxima el value-to-go (con los parámetros actuales de la value function) de cada par estado-accion (representado por un objeto stateaction) y se escoge una acción greedy.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * state: objeto de la clase State que representa el estado actual del problema.\n",
    "            * actions: lista que contiene todas las acciones posibles para state.\n",
    "            * lr: objeto de la clase Ridge de Sklearn que contiene los pesos de la aproximación lineal.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * best_action: acción greedy escogida. Corresponde a una lista que contiene los planes de ruta de cada vehículo.\n",
    "        '''\n",
    "\n",
    "        # se computan los state-action posibles y los values (predichos) asociados a cada uno\n",
    "        best_value = float('-inf')\n",
    "        for action in actions:\n",
    "            # se crea un objeto state_action\n",
    "            state_action = StateAction(state, action)\n",
    "            # se extraen sus features\n",
    "            features = state_action.getFeatures(self.instance)\n",
    "            features = features[1:].T\n",
    "            # se calcula el value\n",
    "            value = lr.predict(features)\n",
    "\n",
    "            # se escoge la acción con mayor value\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "\n",
    "        # se cambia el status de random_cust (del estado justo antes de la transición) a confirmed o a rejected dependiendo si se insertó o no\n",
    "        if state.random_cust is not None:\n",
    "            state.random_cust.status = 'rejected'\n",
    "            state.random_cust.t_confirmed = None\n",
    "            for route in best_action:\n",
    "                # si se insertó el cliente en la acción su status es 'confirmed'\n",
    "                if state.random_cust in route:\n",
    "                    state.random_cust.status = 'confirmed'\n",
    "                    state.random_cust.t_confirmed = state.t\n",
    "                    break\n",
    "        \n",
    "        return best_action\n",
    "\n",
    "\n",
    "    def testLr(self, lr):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que aplica la política de decisión a una realization dada la regresión Ridge previamente entrenada.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * lr: objeto de la clase Ridge de Sklearn que contiene los pesos de la aproximación lineal entrenada.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * test_reward: rewards obtenidos en las realizaciones test aplicando la política CI.\n",
    "            * penalties: valor total de penalización.\n",
    "            * n_penalties: número de clientes atendidos con retraso.\n",
    "            * n_rejects: número de clientes rechazados.\n",
    "            * test_mape: mape obtenido en test.\n",
    "            * test_mse: mse obtenido en test.\n",
    "            * fig: gráfico de la distribución del MAPE y MSE para test.\n",
    "        '''\n",
    "\n",
    "        all_values_obs = []\n",
    "        all_features_obs = []\n",
    "        test_rewards = []\n",
    "        penalties = []\n",
    "        n_penalties = []\n",
    "        n_rejects = []\n",
    "\n",
    "        for realization in copy.deepcopy(self.test_realizations):\n",
    "            # se crea una instancia de state\n",
    "            state = State(self.instance)\n",
    "            # lista para guardar los rewards recibidos por luego de tomar una acción en un estado (rewards de un state_action)\n",
    "            realization_rewards = []\n",
    "            # lista para guardar values observados (o returns) recibidos durante la realizacion\n",
    "            realization_values_obs = []\n",
    "            # lista para guardar los features extraidos de los state_action\n",
    "            realization_features = []\n",
    "            # se define el estado inicial\n",
    "            state.initialState(realization, self.instance)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # si se está en el estado terminal termina el problema\n",
    "                if state.isTerminalState(self.instance):\n",
    "                    break\n",
    "                # se definen las acciones\n",
    "                actions = self.process.computeActions(state, self.instance)\n",
    "                # se aplica take action sobre el conjunto de acciones\n",
    "                action = self.takeActionLr(state, actions, lr, realization)\n",
    "                # se crea un objeto state_action\n",
    "                state_action = StateAction(state, action)\n",
    "                # se extraen las features del state_action\n",
    "                features = state_action.getFeatures(self.instance)\n",
    "                features = features.T.tolist()[0]\n",
    "                features = features[1:]\n",
    "                # se almacenan los features en la lista\n",
    "                realization_features.append(features)\n",
    "                # se aplica la transición aleatoria creando un estado nuevo y un reward\n",
    "                state, reward = self.process.transition(state, action, realization, self.instance)\n",
    "                # se guarda el reward observado\n",
    "                realization_rewards.append(reward)\n",
    "\n",
    "            # a continuación se calcula y almacena el value observado para cada state_action visitado durante la realizacion\n",
    "            realization_values_obs = list(itertools.accumulate(realization_rewards[::-1]))[::-1]\n",
    "            # se agregan los values observados en la realización a la lista de values observados\n",
    "            all_values_obs += realization_values_obs\n",
    "            # se agregan los features observados en la realización a la lista de features observados\n",
    "            all_features_obs += realization_features\n",
    "            # se guarda el reward del episodio\n",
    "            test_rewards.append(np.sum(realization_rewards))\n",
    "            # se guarda el penalty de la realización\n",
    "            penalties.append(sum(cust.penalty for cust in realization))\n",
    "            # se guarda la cantidad de pedidos atrasados\n",
    "            n_penalties.append(sum(cust.penalty > 0 for cust in realization))\n",
    "            # se guarda la cantidad de rechazos de la realización\n",
    "            n_rejects.append(sum(cust.status == 'rejected' for cust in realization))\n",
    "            \n",
    "        # se obtienen los values que fueron predichos\n",
    "        values_pred = lr.predict(all_features_obs)\n",
    "\n",
    "        # se calculan los errores de entrenamiento\n",
    "        test_mape = round(mean_absolute_percentage_error(all_values_obs, values_pred), 2)\n",
    "        test_mse = round(mean_squared_error(all_values_obs, values_pred), 2)\n",
    "\n",
    "        # se calculan los errores individuales para ser graficados\n",
    "        test_mape_list = [round(mean_absolute_percentage_error([value_obs_all], [value_pred]), 2) for value_obs_all, value_pred in zip(all_values_obs, values_pred)]\n",
    "        test_mse_list = [round(mean_squared_error([value_obs_all], [value_pred]), 2) for value_obs_all, value_pred in zip(all_values_obs, values_pred)]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 6))\n",
    "        # se grafica la distribución del MAPE\n",
    "        sns.histplot(test_mape_list, kde=True, ax=axes[0])\n",
    "        median_mape = np.median(test_mape_list)\n",
    "        axes[0].axvline(x=median_mape, color='red')\n",
    "        axes[0].set_title('Distribución de test MAPE')\n",
    "        axes[0].set_xlabel('MAPE')\n",
    "        # se grafica la distribución del MSE\n",
    "        sns.histplot(test_mse_list, kde=True, ax=axes[1])\n",
    "        median_mse = np.median(test_mse_list)\n",
    "        axes[1].axvline(x=median_mse, color='red')\n",
    "        axes[1].set_title('Distribución de test MSE')\n",
    "        axes[1].set_xlabel('MSE')\n",
    "        fig.tight_layout()\n",
    "            \n",
    "        return test_rewards, penalties, n_penalties, n_rejects, test_mape, test_mse, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnPolicyMonteCarlo(Algorithm):\n",
    "\n",
    "    '''\n",
    "    Descripción:\n",
    "\n",
    "        * Subclase que crea un objeto del algortimo AVI. Esta clase contiene la forma una forma de resolver el VRP.\n",
    "\n",
    "    Atributos:\n",
    "\n",
    "        * instance: objeto de la clase Instance que contiene las características del problema.\n",
    "        * process: objeto de la clase Process que contiene el MDP.\n",
    "        * train_realizations: conjunto de realizaciones para entrenar el algoritmo.\n",
    "        * test_realizations: conjunto de realizaciones para testear el algoritmo.\n",
    "    ''' \n",
    "        \n",
    "    def takeAction(self, state, actions, value_function, train=False, epsilon=None):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método para que selecciona una acción en cierto estado a partir de un conjunto de acciones. Para esto, se calcula el value (con los parámetros actuales de la value function) de cada par estado-accion (representado por un objeto state_action) y se escoge una acción con una política epsilon-greedy (en el caso de entrenamiento) y greedy en el caso en que se aplique la política entrenada.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * state: objeto de la clase State que representa el estado actual del problema.\n",
    "            * actions: lista que contiene todas las acciones posibles para state.\n",
    "            * value_function: objeto de la clase ValueFunction que contiene los pesos de la aproximación lineal.\n",
    "            * train: booleano que indica si se está en entrenamiento o testeo, y así determinar si se utiliza o no epsilon greedy.\n",
    "            * epsilon: hiperparámetro de epsilon greedy que indica la probabilidad de tomar una acción random.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * action: entrega la acción escogida (si está en fase de entrenamiento la escoge con epsilon-greedy). Corresponde a una lista que contiene los planes de ruta de cada vehículo.\n",
    "        '''\n",
    "\n",
    "        # se computan los state-action posibles y los values (predichos) asociados a cada uno\n",
    "        best_value = float('-inf')\n",
    "        for action in actions:\n",
    "            # se crea un objeto state_action\n",
    "            state_action = StateAction(state, action)\n",
    "            # se extraen sus features\n",
    "            features = state_action.getFeatures(self.instance)\n",
    "            # se calcula el value\n",
    "            value = value_function.predict(features)\n",
    "            # se almacenan la acciones que da el mayor value\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "\n",
    "        # si se está en fase de entrenamiento se considera una política epsilon-greedy\n",
    "        if train:\n",
    "            # random.seed(1387498)\n",
    "            # con probabilidad epsilon se toma una acción aleatoria\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(actions)\n",
    "            # con probabilidad 1 - epsilon se toma una acción greedy\n",
    "            else:\n",
    "                action = best_action\n",
    "        # si no se está en fase de entrenamiento se toma una acción greedy\n",
    "        else:\n",
    "            action = best_action\n",
    "\n",
    "        # se cambia el status de random_cust (del estado justo antes de la transición) a confirmed o a rejected dependiendo si se insertó o no\n",
    "        if state.random_cust is not None:\n",
    "            state.random_cust.status = 'rejected'\n",
    "            for route in action:\n",
    "                if state.random_cust.id in [cust.id for cust in route]:\n",
    "                    state.random_cust.status = 'confirmed'\n",
    "                    state.random_cust.t_confirmed = state.t\n",
    "                    break\n",
    "\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train(self, value_function, epsilon):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que aplica el algoritmo On policy Monte Carlo control (AVI) para encontrar una aproximación de la value function.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * value_function: objeto de la clase ValueFunction para ser entrenado.\n",
    "            * epsilon: hiperparámetro de epsilon greedy que indica la probabilidad de tomar una acción random.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * value_function: corresponde a un objeto de la clase ValueFunction que contiene un conjunto de pesos entrenados para la regresión que representa la aproximación lineal de la value function.\n",
    "            * train_rewards: rewards para las realizaciones train.\n",
    "            * penalties: valor total de penalización.\n",
    "            * n_penalties: número de clientes atendidos con retraso.\n",
    "            * n_rejects: número de clientes rechazados.\n",
    "            * mape_list: lista de errores mape para cada episodio de entrenamiento.\n",
    "            * mse_list: lista de errores mse para cada episodio de entrenamiento.\n",
    "            * fig: gráfico que contiene el MAPE y MSE para cada episodio de entrenamiento.\n",
    "        '''\n",
    "        \n",
    "        mape_list = []\n",
    "        mse_list = []\n",
    "        train_rewards = []\n",
    "        penalties = []\n",
    "        n_penalties = []\n",
    "        n_rejects = []\n",
    "\n",
    "        for realization in copy.deepcopy(self.train_realizations):\n",
    "            # se crea una instancia de state\n",
    "            state = State(self.instance)\n",
    "            # lista para guardar los rewards recibidos por luego de tomar una acción en un estado (rewards de un state_action)\n",
    "            realization_rewards = []\n",
    "            # listas para guardar los values observados y los values predichos de la realización\n",
    "            realization_values_obs = []\n",
    "            realization_values_pred = []\n",
    "            # lista para guardar los features extraidos de los state_action\n",
    "            realization_features = []\n",
    "            # se define el estado inicial\n",
    "            state.initialState(realization, self.instance)\n",
    "\n",
    "            while True:\n",
    "                # si se está en el estado terminal termina el problema\n",
    "                if state.isTerminalState(self.instance):\n",
    "                    break\n",
    "                # se definen las acciones\n",
    "                actions = self.process.computeActions(state, self.instance)\n",
    "                # se aplica take action sobre el conjunto de acciones\n",
    "                action = self.takeAction(state, actions, value_function, train=True, epsilon=epsilon)\n",
    "                # se crea un objeto state_action\n",
    "                state_action = StateAction(state, action)\n",
    "                # se extraen los features del state_action\n",
    "                features = state_action.getFeatures(self.instance)\n",
    "                # se guardan los features\n",
    "                realization_features.insert(0, features)\n",
    "                # se aplica la transición aleatoria creando un estado nuevo y un reward\n",
    "                state, reward = self.process.transition(state, action, realization, self.instance)\n",
    "                # se guarda el reward observado\n",
    "                realization_rewards.append(reward)\n",
    "\n",
    "            # a continuación se calcula y almacena el value observado para cada state_action visitado durante la realizacion\n",
    "            realization_values_obs = list(itertools.accumulate(realization_rewards[::-1]))\n",
    "            # se predice el value y se actualiza \n",
    "            for features, value_obs in zip(realization_features, realization_values_obs):\n",
    "                # se calcula el value predicho para los features observados en el state_action\n",
    "                value_pred = value_function.predict(features)\n",
    "                # a partir del value observado se actualizan los pesos de la regresión\n",
    "                value_function.updateWeights(features, value_pred, value_obs)\n",
    "                # se guardan los valores predichos\n",
    "                realization_values_pred.append(value_pred)\n",
    "\n",
    "            # se guarda el reward del episodio\n",
    "            train_rewards.append(np.sum(realization_rewards))\n",
    "            # se guarda el penalty de la realización\n",
    "            penalties.append(sum(cust.penalty for cust in realization))\n",
    "            # se guarda la cantidad de pedidos atrasados\n",
    "            n_penalties.append(sum(cust.penalty > 0 for cust in realization))\n",
    "            # se guarda la cantidad de rechazos de la realización\n",
    "            n_rejects.append(sum(cust.status == 'rejected' for cust in realization))\n",
    "            \n",
    "            # se calcula el error de la predicción y se guarda en las listas de errores\n",
    "            mape = mean_absolute_percentage_error(realization_values_obs, realization_values_pred)\n",
    "            mse = mean_squared_error(realization_values_obs, realization_values_pred)\n",
    "            mape_list.append(mape)\n",
    "            mse_list.append(mse)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize = (10,5))\n",
    "        x = np.arange(0, len(mape_list))\n",
    "        # se grafica el mape\n",
    "        y_mape = mape_list\n",
    "        axes[0].plot(x, y_mape)\n",
    "        axes[0].set_xlim(left=0)\n",
    "        axes[0].set_ylim(bottom=0)\n",
    "        axes[0].set_ylabel('Mean Absolute Percentage Error')\n",
    "        axes[0].set_xlabel('Run')\n",
    "        axes[0].set_title('Train MAPE')\n",
    "        # se grafica el mse\n",
    "        y_mse = mse_list\n",
    "        axes[1].plot(x, y_mse)\n",
    "        axes[1].set_xlim(left=0)\n",
    "        axes[1].set_ylim(bottom=0)\n",
    "        axes[1].set_ylabel('Mean Squared Error')\n",
    "        axes[1].set_xlabel('Run')\n",
    "        axes[1].set_title('Train MSE')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        return value_function, train_rewards, penalties, n_penalties, n_rejects, mape_list, mse_list, fig\n",
    "    \n",
    "\n",
    "    def test(self, value_function):\n",
    "\n",
    "        '''\n",
    "        Descripción:\n",
    "\n",
    "            * Método que aplica la política de decisión a una realization dada la value function previamente entrenada.\n",
    "\n",
    "        Parámetros:\n",
    "\n",
    "            * value_function: objeto de la clase ValueFunction que permite acceder a la política de decisión a través de la aproximación de los value de cada par estado-acción. Corresponde a la value function entrenada.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            * test_reward: rewards obtenidos en las realizaciones test aplicando la política entrenada.\n",
    "            * penalties: valor total de penalización.\n",
    "            * n_penalties: número de clientes atendidos con retraso.\n",
    "            * n_rejects: número de clientes rechazados.\n",
    "            * test_mape: mape obtenido en test.\n",
    "            * test_mse: mse obtenido en test.\n",
    "            * fig: gráfico de la distribución del MAPE y MSE para test.\n",
    "        '''\n",
    "\n",
    "        all_values_obs = []\n",
    "        all_features_obs = []\n",
    "        test_rewards = []\n",
    "        penalties = []\n",
    "        n_penalties = []\n",
    "        n_rejects = []\n",
    "        \n",
    "        for realization in copy.deepcopy(self.test_realizations):\n",
    "            # se crea una instancia de state\n",
    "            state = State(self.instance)\n",
    "            # lista para guardar los rewards recibidos por luego de tomar una acción en un estado (rewards de un state_action)\n",
    "            realization_rewards = []\n",
    "            # lista para guardar los features extraidos de los state_action\n",
    "            realization_features = []\n",
    "            # se define el estado inicial\n",
    "            state.initialState(realization, self.instance)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # si se está en el estado terminal termina el problema\n",
    "                if state.isTerminalState(self.instance):\n",
    "                    break\n",
    "                # se definen las acciones\n",
    "                actions = self.process.computeActions(state, self.instance)\n",
    "                # se aplica take action sobre el conjunto de acciones\n",
    "                action = self.takeAction(state, actions, value_function)\n",
    "                # se crea un objeto state_action\n",
    "                state_action = StateAction(state, action)\n",
    "                # se extraen las features del state_action\n",
    "                features = state_action.getFeatures(self.instance)\n",
    "                # se almacenan los features en la lista\n",
    "                realization_features.append(features)\n",
    "\n",
    "                # se aplica la transición aleatoria creando un estado nuevo y un reward\n",
    "                state, reward = self.process.transition(state, action, realization, self.instance)\n",
    "                # se guarda el reward observado en la primera posición de lista\n",
    "                realization_rewards.append(reward)\n",
    "\n",
    "            # a continuación se calcula y almacena el value observado para cada state_action visitado durante la realizacion\n",
    "            realization_values_obs = list(itertools.accumulate(realization_rewards[::-1]))[::-1]\n",
    "            # se agregan los values observados en la realización a la lista de values observados\n",
    "            all_values_obs += realization_values_obs\n",
    "            # se agregan los features observados en la realización a la lista de features observados\n",
    "            all_features_obs += realization_features\n",
    "            # se guarda el reward del episodio\n",
    "            test_rewards.append(np.sum(realization_rewards))\n",
    "            # se guarda el penalty de la realización\n",
    "            penalties.append(sum(cust.penalty for cust in realization))\n",
    "            # se guarda la cantidad de pedidos atrasados\n",
    "            n_penalties.append(sum(cust.penalty > 0 for cust in realization))\n",
    "            # se guarda la cantidad de rechazos de la realización\n",
    "            n_rejects.append(sum(cust.status == 'rejected' for cust in realization))\n",
    "            \n",
    "        # a partir de los features observados se predicen los values\n",
    "        values_pred = [value_function.predict(features) for features in all_features_obs]\n",
    "\n",
    "        # se calculan los errores de entrenamiento\n",
    "        test_mape = round(mean_absolute_percentage_error(all_values_obs, values_pred), 2)\n",
    "        test_mse = round(mean_squared_error(all_values_obs, values_pred), 2)\n",
    "\n",
    "        # se calculan los errores individuales para ser graficados\n",
    "        test_mape_list = [round(mean_absolute_percentage_error([value_obs_all], [value_pred]), 2) for value_obs_all, value_pred in zip(all_values_obs, values_pred)]\n",
    "        test_mse_list = [round(mean_squared_error([value_obs_all], [value_pred]), 2) for value_obs_all, value_pred in zip(all_values_obs, values_pred)]\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8, 6))\n",
    "        # se grafica la distribución del MAPE\n",
    "        sns.histplot(test_mape_list, kde=True, ax=axes[0])\n",
    "        median_mape = np.median(test_mape_list)\n",
    "        axes[0].axvline(x=median_mape, color='red')\n",
    "        axes[0].set_title('Distribución de test MAPE')\n",
    "        axes[0].set_xlabel('MAPE')\n",
    "        # se grafica la distribución del MSE\n",
    "        sns.histplot(test_mse_list, kde=True, ax=axes[1])\n",
    "        median_mse = np.median(test_mse_list)\n",
    "        axes[1].axvline(x=median_mse, color='red')\n",
    "        axes[1].set_title('Distribución de test MSE')\n",
    "        axes[1].set_xlabel('MSE')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        return test_rewards, penalties, n_penalties, n_rejects, test_mape, test_mse, fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crean las instancias diseñadas previamente\n",
    "\n",
    "Nota: para probar realizar los diferentes experimentos se debe descomentar la instancia que se quiere resolver y ejecutar los algoritmos que están más abajo con los parámetros deseados de N° simulaciones de entrenamiento y prueba, lambda y epsilon. Cabe destacar que los resultados experimentales mostrados en el documento de tesis utilizan los simulation seeds que están establecidos en la ejecución de los algoritmos, es decir simulation seed = 9 para simulaciones de entrenamiento y simulation seed = 5 para simulaciones de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea todas las instancia diseñadas, un Proceso y objetos de los Algoritmos de resolución (CI y OPMC)\n",
    "\n",
    "# INSTANCIAS DE 2 VEHÍCULOS\n",
    "\n",
    "instance = Instance(A_x=6, \n",
    "                    A_y=6, \n",
    "                    n_vehicles=2, \n",
    "                    n_cust=20,\n",
    "                    dod=0.99, \n",
    "                    t_max=7, \n",
    "                    t_service=5, \n",
    "                    t_window=30, \n",
    "                    cust_categories=[1], \n",
    "                    penalty_factor=15, \n",
    "                    t_delta=5, \n",
    "                    vel_mean=30,\n",
    "                    vel_std=0, \n",
    "                    idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "                    stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=2, \n",
    "#                     n_cust=40,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=2, \n",
    "#                     n_cust=50,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=2, \n",
    "#                     n_cust=60,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=2, \n",
    "#                     n_cust=80,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# INSTANCIAS DE 4 VEHÍCULOS\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=4, \n",
    "#                     n_cust=40,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=4, \n",
    "#                     n_cust=80,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=4, \n",
    "#                     n_cust=100,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=4, \n",
    "#                     n_cust=120,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=4, \n",
    "#                     n_cust=160,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# INSTANCIAS DE 6 VEHÍCULOS\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=6, \n",
    "#                     n_cust=60,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=6, \n",
    "#                     n_cust=120,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=6, \n",
    "#                     n_cust=150,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=6, \n",
    "#                     n_cust=180,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=6, \n",
    "#                     n_cust=240,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# INSTANCIAS DE 8 VEHÍCULOS\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=8, \n",
    "#                     n_cust=80,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=8, \n",
    "#                     n_cust=160,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=8, \n",
    "#                     n_cust=200,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=8, \n",
    "#                     n_cust=240,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=8, \n",
    "#                     n_cust=320,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# INSTANCIAS DE 10 VEHÍCULOS\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=10, \n",
    "#                     n_cust=100,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=10, \n",
    "#                     n_cust=200,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=10, \n",
    "#                     n_cust=250,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=10, \n",
    "#                     n_cust=300,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# instance = Instance(A_x=6, \n",
    "#                     A_y=6, \n",
    "#                     n_vehicles=10, \n",
    "#                     n_cust=400,\n",
    "#                     dod=0.99, \n",
    "#                     t_max=7, \n",
    "#                     t_service=5, \n",
    "#                     t_window=30, \n",
    "#                     cust_categories=[1], \n",
    "#                     penalty_factor=15, \n",
    "#                     t_delta=5, \n",
    "#                     vel_mean=30,\n",
    "#                     vel_std=0, \n",
    "#                     idle_points_pos=[(1.5, 1.5), (4.5, 1.5), (3.0, 5.0)], \n",
    "#                     stoc_travel_time=False)\n",
    "\n",
    "# se crea un objeto Process que contiene el MDP\n",
    "process = Process()\n",
    "\n",
    "# se crea un objeto de CI\n",
    "cheapest_insertion = CheapestInsertion(instance, process)\n",
    "# se crea un objeto de OPMC\n",
    "monte_carlo = OnPolicyMonteCarlo(instance, process)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheapest Insertion (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crean las realizaciones de testeo para el algoritmo\n",
    "cheapest_insertion.simulateTestRealizations(N=1000, simulation_seed=5)\n",
    "print('El número de clientes en las realizaciones es:', round(np.mean([len(realization) for realization in cheapest_insertion.test_realizations]), 2))\n",
    "# testeando la política básica\n",
    "ci_test_rewards, ci_test_penalties, ci_test_npenalties, ci_test_nrejects = cheapest_insertion.test()\n",
    "print('El reward promedio para test es:', round(np.mean(ci_test_rewards), 2))\n",
    "print('El penalty promedio por pedidos atrasados fue:', round(np.mean(ci_test_penalties), 2))\n",
    "print('El número promedio de pedidos con atraso fue:', round(np.mean(ci_test_npenalties), 2))\n",
    "print('El número promedio de pedidos rechazados fue:', round(np.mean(ci_test_nrejects), 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Policy Monte Carlo Control (Train y Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crean las realizaciones de entrenamiento para el algoritmo\n",
    "monte_carlo.simulateTrainRealizations(N=10000, simulation_seed=9)\n",
    "print('El número de clientes en las realizaciones es:', round(np.mean([len(realization) for realization in monte_carlo.train_realizations]), 2))\n",
    "\n",
    "# se crea una instancia de value function\n",
    "mc_initial_value_function = ValueFunction()\n",
    "# se inicializa RLS para la aproximación de la value function\n",
    "mc_initial_value_function.initializeRecursiveLeastSquares(lambd=1000)\n",
    "\n",
    "# entrenando el modelo con el método que utiliza RLS\n",
    "mc_trained_value_function, mc_train_rewards, mc_train_penalties, mc_train_npenalties, mc_train_nrejects, mc_mape_list, mc_mse_list, mc_fig_train_errors = monte_carlo.train(mc_initial_value_function, epsilon=0.05)\n",
    "print('Los pesos resultantes para la value function son:', mc_trained_value_function.weights.T)\n",
    "print('\\nEl reward promedio para train es:', round(np.mean(mc_train_rewards), 2))\n",
    "print('El penalty promedio por pedidos atrasados fue:', round(np.mean(mc_train_penalties), 2))\n",
    "print('El número promedio de pedidos con atraso fue:', round(np.mean(mc_train_npenalties), 2))\n",
    "print('El número promedio de pedidos rechazados fue:', round(np.mean(mc_train_nrejects), 2))\n",
    "print('El train mape final es: {} y el train mse final es: {}'.format(round(mc_mape_list[-1], 2), round(mc_mse_list[-1], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crean las realizaciones de test para el algoritmo\n",
    "monte_carlo.simulateTestRealizations(N=1000, simulation_seed=5)\n",
    "print('El número de clientes en las realizaciones es:', round(np.mean([len(realization) for realization in monte_carlo.test_realizations]), 2))\n",
    "\n",
    "# testeando la política encontrada en train con el método RLS\n",
    "mc_test_rewards, mc_test_penalties, mc_test_npenalties, mc_test_nrejects, mc_test_mape, mc_test_mse, mc_fig_test_errors = monte_carlo.test(mc_trained_value_function)\n",
    "print('El reward promedio para test es:', round(np.mean(mc_test_rewards), 2))\n",
    "print('El penalty promedio por pedidos atrasados fue:', round(np.mean(mc_test_penalties), 2))\n",
    "print('El número promedio de pedidos con atraso fue:', round(np.mean(mc_test_npenalties), 2))\n",
    "print('El número promedio de pedidos rechazados fue:', round(np.mean(mc_test_nrejects), 2))\n",
    "print('El test mape es: {} y el test mse es: {}'.format(mc_test_mape, mc_test_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c13fa53d9ca57d7eb06b3bded45cae9d1d4fccba06703c1e05380e65eacdcc4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
